---
title: "CRPD Data Prep"
author: "Theodore Ochieng"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Load Packages}
library(tidyverse)# Also loads dplyr, ggplot2, and haven
library(tidytext)
library(readtext)
```

# Setup
```{r Function: countByGrouping}
countByGrouping <- function(dataset, varToGroup) {
  dataset %>% 
    count({{ varToGroup }}, sort = T) %>% 
    mutate(prop = round(n/sum(n), 4)) %>% 
    tibble() %>%
    return()
}
```



# Part 1: Gather and Clean Data
## Import Data
```{r Import State Reports}
CRPD_StateReports <- readtext("CRPD_Data/State Reports/*",  # change this to point to CRPD State Reports directory
                              docvarsfrom = "filenames", 
                              dvsep="_", 
                              docvarnames = c("Country", "Session", "Year"))

CRPD_StateReports <- CRPD_StateReports %>% 
  mutate(doc_id = str_remove_all(doc_id, ".pdf"))

summary(CRPD_StateReports)
head(CRPD_StateReports) %>% 
  as_tibble()
```


```{r Import Geographic Data}
un_geo_info <- read_csv("CRPD_Data/Metadata/Resources/UN_Geographic_Info.csv")
head(un_geo_info)

opt_protocol_geo_info <- read_csv("CRPD_Data/Metadata/Resources/opt_protocol_geo_info.csv")
head(opt_protocol_geo_info)
```


```{r Merge Georgapic Info}
CRPD_StateReports <- CRPD_StateReports %>% 
  left_join(select(un_geo_info, country_area_code, region, sub_region, regional_group), by = c("Country" = "country_area_code")) %>% 
  rename(Region = region,
         SubRegion = sub_region,
         RegionalGroup = regional_group) %>% 
  left_join(select(opt_protocol_geo_info, country_area_code, opt_protocol), by = c("Country" = "country_area_code")) %>% 
  rename(OptionalProtocol = opt_protocol)

head(CRPD_StateReports)%>% 
  as_tibble()
```

```{r Import Income Level Data}
income_level <- readxl::read_xlsx("CRPD_Data/Metadata/Resources/World Bank Income Groups.xlsx") %>% 
  janitor::clean_names(case = "upper_camel")
head(income_level)
```


```{r Merge Income Level Data}
CRPD_StateReports <- CRPD_StateReports %>% 
  left_join(select(income_level, Code, IncomeGroup), by = c("Country" = "Code"))

head(CRPD_StateReports)%>% 
  as_tibble()
```


```{r Records by Grouping}
countByGrouping(CRPD_StateReports, Year)
countByGrouping(CRPD_StateReports, Session)
countByGrouping(CRPD_StateReports, Country)
countByGrouping(CRPD_StateReports, Region)
countByGrouping(CRPD_StateReports, SubRegion)
countByGrouping(CRPD_StateReports, RegionalGroup)
countByGrouping(CRPD_StateReports, IncomeGroup)
```


```{r Check NAs}
# Get metadata about NAs 
CRPD_StateReports %>% 
  naniar::miss_var_summary() %>% 
  mutate(pct_miss = round(pct_miss, 2))
```


```{r Detect Language}
CRPD_StateReports <- CRPD_StateReports %>% 
  mutate(cld3_lang = cld3::detect_language(text),
         .after = text)

head(CRPD_StateReports) %>% 
  tibble()

CRPD_StateReports %>% 
  count(cld3_lang, sort = T) %>% 
  mutate(prop = round(n/sum(n), 4)) %>% 
  tibble()
```


```{r See Non-English Reports}
CRPD_StateReports %>% 
  filter(cld3_lang != "en") %>% 
  tibble()
```


```{r Select English Reports}
CRPD_StateReports_en <- CRPD_StateReports %>% 
  filter(cld3_lang == "en") %>% 
  select(-cld3_lang)

head(CRPD_StateReports_en)
glimpse(CRPD_StateReports_en)
```


```{r Clean Data}
CRPD_StateReports_en <- CRPD_StateReports_en %>%
  # standardize apostrophes
  mutate(text = str_replace_all(text, "’", "'")) %>% 
  # remove numbers
  mutate(text = str_replace_all(text, '[0-9]', "")) %>%
  # remove domain names
  mutate(text = str_replace_all(text, '[A-Za-z0-9]+\\.[A-Za-z0-9]+\\.[A-Za-z0-9]+', "")) %>%
  mutate(text = str_replace_all(text, '[A-Za-z0-9]+\\.[A-Za-z0-9]+', "")) %>%  # e.g. avian.com
  # remove underscore patterns
  mutate(text = str_replace_all(text, '[A-z]+[_][A-z]+', "")) %>%
  mutate(text = str_replace_all(text, '[A-z]+[_]', "")) %>%
  mutate(text = str_replace_all(text, '[_][A-z]+', ""))
```


```{r Save Dataset}
write_csv(CRPD_StateReports_en, "CRPD_Data/CRPD_StateReports_English_V2.csv")
```


# Part 2: Create Stopwords
```{r Stock Stopwords}
# ---- Load Individual Stock Stopwords ----
library(stopwords)

# English stopwords - 1,298 unique words
en_stopwords <-
  c(stopwords("en", source = "stopwords-iso"),
    stopwords("en", source = "snowball"),
    stopwords("en", source = "smart")) %>%
  unique() %>%
  as.data.frame() %>%
  rename("word" = ".") %>%
  mutate(lexicon = "stopwords_pkg_english") %>%
  rbind(distinct(tidytext::stop_words, word, .keep_all = TRUE)) %>% # get English stopwords from tidytext
  distinct(word, .keep_all = TRUE)

# Spanish stopwords - 732 unique words
es_stopwords <-
  c(stopwords("es", source = "stopwords-iso"),
    stopwords("es", source = "snowball")) %>%
  unique() %>%
  as.data.frame() %>%
  rename("word" = ".") %>%
  mutate(lexicon = "stopwords_pkg_spanish")

# French stopwords - 689 unique words
fr_stopwords <-
  c(stopwords("fr", source = "stopwords-iso"),
    stopwords("fr", source = "snowball")) %>%
  unique() %>%
  as.data.frame() %>%
  rename("word" = ".") %>%
  mutate(lexicon = "stopwords_pkg_french")

# Russian stopwords - 559 unique words
ru_stopwords <-
  c(stopwords("ru", source = "stopwords-iso"),
    stopwords("ru", source = "snowball")) %>%
  unique() %>%
  as.data.frame() %>%
  rename("word" = ".") %>%
  mutate(lexicon = "stopwords_pkg_russian")

# ---- Combine Stock Stopwords ----
stock.stopwords <-
  rbind(en_stopwords, es_stopwords, fr_stopwords, ru_stopwords) %>%
  # standardize apostrophes
  mutate(word = str_replace_all(word, "’", "'")) %>%
  # remove duplicate words (130 words removed)
  distinct(word, .keep_all = TRUE)

head(stock.stopwords)

# remove individual stock stopwords
rm(en_stopwords, es_stopwords, fr_stopwords, ru_stopwords)

detach("package:stopwords", unload = TRUE)
```


## Custom Stopwords
```{r Unigrams}
# ---- Fetch Unigrams ----
unigrams <- read_csv("CRPD_Data/Metadata/Metadata/unigram_counts_v2.csv")
head(unigrams)

# ---- Select by Quantile ----
# Filter unigrams for words only in the 99th percentile
unigrams_quant <- unigrams %>% 
  filter(n >= quantile(n, 0.995))

head(unigrams_quant)
```


```{r Custom Stopwords}
custom.stopwords <- c(tolower(month.name[c(1:12)]),
                      unigrams_quant$word,
                      "united", "nation", "nations", "unedited", "version", "versions", "advance",
                      "suesaec", "dacuvecu", "ecnavda",
                      "official", "oficial", "inter", "alia", "karonga", "mzimba", "mchinji", "lilongwe", 
                      "salima", "machinga", "mulanje", "chiradzulu", "nsanje",  
                      "english", "spanish", "russian", "french", "arabic", "original", "vis", "à", 
                      "sites", "default", "file", "files", "wp", "content", "upload", "uploads",
                      "http", "https", "eur", "lexuriserv", "uri")

custom.stopwords <- as.data.frame(custom.stopwords) %>% 
  rename("word" = `custom.stopwords`) %>%
  # add lexicon in case we decide to rbind with dataframe stop_words
  mutate(lexicon = "Custom_CRPD")

custom.stopwords
```


```{r Create Combined Stopwords}
# Merge stock and custom stopwords
combined.stop_words <- 
  rbind(stock.stopwords, custom.stopwords) %>% 
  # remove duplicate words (3 words removed)
  distinct(word, .keep_all = TRUE)

combined.stop_words
```


```{r Save Dataset}
# write_csv(combined.stop_words, "CRPD_Data/combined_stopwords.csv")
```


```{r Remove Objects}
# rm(stock.stopwords, custom.stopwords, unigrams, unigrams_quant)
```

